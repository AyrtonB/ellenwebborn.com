---
layout: post
title: Alternative formulas for the standardized mean difference
date: April 26, 2016
tags: [meta-analysis, effect-sizes, distribution-theory]
---

The standardized mean difference (SMD) is surely one of the best known and most widely used effect size metrics used in meta-analysis. In generic terms, the SMD parameter is defined as the difference in population means between two groups (often this difference represents the effect of some intervention), scaled by the population standard deviation of the outcome metric. Estimates of the SMD can be obtained from a wide variety of experimental designs, ranging from simple, completely randomized designs, to repeated measures designs, to cluster-randomized trials.

There's some nuance involved in figuring out how to calculate estimates of the SMD from each design, mostly to do with exactly what sort of standard deviation to use in the denominator of the effect size. I'll leave that discussion for another day. Here, I'd like to look at the question of how to estimate the sampling variance of the SMD. An estimate of the sampling variance is needed in order to meta-analyze a collection of effect sizes, and so it's an important consideration for any research synthesis. However, the standard textbook treatments of effect size calculations cover this question only for a limited number of simple cases. I'd like to suggest a different, more general way of thinking about it, which provides a way to estimate the SMD and its variance in non-standard cases (and also leads to slight differences in the standard ones). All of this might be old hat for seasoned synthesists, but I hope it might be useful for students and researchers just getting started with meta-analysis.

To start, let me review (regurgitate?) the standard textbook presentation.

### SMD from a simple, independent groups design

Textbook presentations of the SMD estimator almost always start by introducing the estimator in the context of a __simple, independent groups design__. Call the groups T and C, the sample sizes $n_T$ and $n_C$, the sample means $\bar{y}_T$ and $\bar{y}_C$, and the sample variances $s_T^2$ and $s_C^2$. A basic moment estimator of the SMD is then 

$$
d = \frac{\bar{y}_T - \bar{y}_C}{s_p}
$$

where $s_p^2 = \frac{\left(n_T - 1\right)s_T^2 + \left(n_C - 1\right) s_C^2}{n_T + n_C - 2}$ is a pooled estimator of the population variance. The standard estimator for the sampling variance of $d$ is 

$$
V_d = \frac{n_T + n_C}{n_T n_C} + \frac{d^2}{2\left(n_T + n_C - 2\right)},
$$

or some slight variant thereof. This estimator is based on a delta-method approximation for the asymptotic variance of $d$. 

It is well known that $d$ has a small sample bias that depends on sample sizes. Letting

$$
J(x) = 1 - \frac{3}{4x - 1},
$$

the corrected estimator is 

$$
g = J\left(n_T + n_C - 2\right) \times d,
$$ 

and is often referred to as Hedges' $g$ because it was proposed in [Hedges (1981)](http://doi.org/10.3102/10769986006002107). Some meta-analysts use $V_d$, but with $d^2$ replaced by $g^2$, as an estimator of the large-sample variance of $g$; others use 

$$
V_g = J^2\left(n_T + n_C - 2\right) \left(\frac{n_T + n_C}{n_T n_C} + \frac{g^2}{2\left(n_T + n_C - 2\right)}\right).
$$

[Viechtbauer (2007)](http://doi.org/10.3102/1076998606298034) provides further details on variance estimation and confidence intervals.

### A general formula for $g$ and its sampling variance

The above formulas are certainly useful, but in practice meta-analyses often include studies that use other, sometimes more complex designs. 
Good textbook presentations also cover computation of $g$ and its variance for some other cases (e.g., Borenstein, 2009, also covers one-group pre/post designs and analysis of covariance). Less careful presentations only cover the simple, independent groups design and thus may inadvertently leave the impression that the variance estimator $V_d$ works in general. With other types of studies, $V_d$ can be a wildly biased estimator of the actual sampling variance of $d$, because it is derived under the assumption that the numerator of $d$ is estimated as the difference in means of two simple random samples. In some designs (e.g., ANCOVA designs, randomized block designs, repeated measures designs), the treatment effect estimate will be much more precise than this; in other designs (e.g., cluster-randomized trials), it will be less precise. 

Here's (what I think is) a more useful way to think about the sampling variance of $d$. Let's suppose that we have an unbiased estimator for the difference in means that goes into the numerator of the SMD. Call this estimator $b$, its sampling variance $\text{Var}(b)$, and its standard error $se_{b}$. Also suppose that we have an unbiased (or reasonably close-to-unbiased) estimator of the population variance of the outcome, the square root of which goes into the denominator of the SMD. Call this estimator $S^2$, with expectation $\text{E}\left(S^2\right) = \sigma^2$ and sampling variance $\text{Var}(S^2)$. Finally, suppose that $b$ and $S^2$ are independent (which will often be a pretty reasonable assumption). A delta-method approximation for the sampling variance of $d = b / S$ is then 

$$
\text{Var}\left(d\right) \approx \frac{\text{Var}(b)}{\sigma^2} + \frac{\delta^2}{2 \nu},
$$

where $\nu = 2 \left[\text{E}\left(S^2\right)\right]^2 / \text{Var}\left(S^2\right)$. Plugging in sample estimates of the relevant parameters provides a reasonable estimator for the sampling variance of $d$:

$$
V_d = \left(\frac{se_b}{S}\right)^2 + \frac{d^2}{2 \nu}.
$$

For some designs, the degrees of freedom $\nu$ depend only on sample sizes, and thus can be calculated exactly. For some other designs, $\nu$ must be estimated. Furthermore, a small-sample correction for the bias of $d$ is given by 

$$
g = J(\nu) \times d.
$$

This small-sample correction is based on a Satterthwaite-type approximation to the distribution of $d$. 

Here's another way to express the variance estimator for $d$: 

$$
V_d = d^2 \left(\frac{1}{t^2} + \frac{1}{2 \nu}\right),
$$

where $t$ is the test statistic corresponding to the hypothesis test for no difference between groups. I've never seen that formula in print before, but it could be convenient if an article reports the $t$ statistic (or $F = t^2$ statistic).

### Non-standard estimators of $d$

The advantage of this formulation of $d$, $g$, and $V_d$ is that it can be applied in quite a wide variety of circumstances, including cases that aren't usually covered in textbook treatments. Rather than having to use separate formulas for every design under the sun, the same formulas apply throughout. All that changes are the components of the formulas: the scaled standard error $se_b / S$ and the degrees of freedom $\nu$. A bunch of examples:

#### Independent groups with different variances

Suppose that we're looking at two independent groups but do not want to assume that their variances are the same. In this case, it would make sense to standardize the difference in means by the control group standard deviation (without pooling), so that $d = \left(\bar{y}_T - \bar{y}_C\right) / s_C$. Since $s_C^2$ has $\nu = n_C - 1$ degrees of freedom, the small-sample bias correction will then need to be $J(n_C - 1)$. The scaled standard error will be

$$
\frac{se_b}{S} = \sqrt{\frac{s_T^2}{s_C^2 n_T} + \frac{1}{n_C}}.
$$

This is then everything that we need to calculate $V_d$, $g$, $V_g$, etc.

#### Multiple independent groups

Suppose that the study involves $K - 1$ treatment groups, 1 control group, and $N$ total participants. If the meta-analysis will include SMDs comparing _each_ treatment group to the control group, it would make sense to pool the sample variance across all $K$ groups rather than just the pair of groups, so that a common estimate of scale is used across all the effect sizes. The pooled standard deviation is then calculated as 

$$
s_p^2 = \frac{1}{N - K} \sum_{k=0}^K (n_k - 1) s_k^2.
$$

For a comparison between treatment group $k$ and the control group, we would then use 

$$
d = \frac{\bar{y}_k - \bar{y}_C}{s_p}, \qquad \nu = N - K, \qquad \frac{se_b}{S} = \sqrt{\frac{1}{n_C} + \frac{1}{n_k}},
$$

where $n_k$ is the sample size for treatment group $k$ (cf. Gleser & Olkin, 2009). 

#### Single group, pre-test post-test design 

Suppose that a study involves taking pre-test and post-test measurements on a single group of $n$ participants. Borenstein (2009) recommends calculating the standardized mean difference for this study as the difference in means between the post-test and pre-test, scaled by the pooled (across pre- and post-test measurements) standard deviation. With obvious notation:

$$
d = \frac{\bar{y}_{post} - \bar{y}_{pre}}{s_p}, \qquad \text{where} \qquad s_p^2 = \frac{1}{2}\left(s_{pre}^2 + s_{post}^2\right).
$$

In this design,

$$
\frac{se_b}{S} = \frac{2(1 - r)}{n},
$$

where $r$ is the sample correlation between the pre- and post-tests. The remaining question is what to use for $\nu$. Borenstein (2009) uses $\nu = n - 1$, which amounts to treating $r$ as constant. My previous post [on the sampling covariance of sample variances]({{site.url}}/distribution-of-sample-variances/) gave the result that $\text{Var}(s_p^2) = \sigma^4 (1 + \rho^2) / (n - 1)$, which would instead suggest using 

$$
\nu = \frac{2 (n - 1)}{1 + r^2}. 
$$

This formula will tend to give slightly larger degrees of freedom, but probably won't be that discrepant from Borenstein's approach except in quite small samples. It would be interesting to investigate which approach is better in small samples (i.e., leading to less biased estimates of the SMD and more accurate estimates of sampling variance, and by how much), although its possible than neither is all that good because the variance estimator is based on a large-sample approximation.

#### Two group, pre-test post-test design: ANCOVA estimation

Suppose that a study involves taking pre-test and post-test measurements on two groups of participants, with sample sizes $n_T$ and $n_C$ respectively. One way to analyze this design is via ANCOVA using the pre-test measure as the covariate, so that the treatment effect estimate is the difference in adjusted post-test means. In this design, the scaled standard error will be approximately

$$
\frac{se_b}{S} = \frac{(n_C + n_T)(1 - r^2)}{n_C n_T},
$$

where $r$ is the pooled, within-group sample correlation between the pre-test and the post-test measures. Alternately, if $se_b$ is provided then the scaled standard error could be calculated directly.

Borenstein (2009) suggests calculating $d$ as the difference in adjusted means, scaled by the pooled sample variances on the post-test measures. The post-test pooled sample variance will have the same degrees of freedom as in the two-sample t-test case: $\nu = n_C + n_T - 2$. (Borenstein instead uses $\nu = n_C + n_T - 2 - q$, where $q$ is the number of covariates in the analysis, but again this won't usually make much difference unless the total sample size is really small.)

Scaling by the pooled post-test sample variance isn't the only reasonable way to estimate the SMD though. If the covariate is a true pre-test, then why not scale by the pooled pre-test sample variance instead? To do so, you would need to calculate $se_b / S$ directly and again use $\nu = n_C + n_T - 2$. If it's reasonable to assume that the pre- and post-test population variances are equal, then another alternative would be to pool across the pre-test _and_ post-test sample variances in each group. Using this approach, you would again need to calculate $se_b / S$ directly and then use

$$
\nu = \frac{2(n_C + n_T - 2)}{1 + r^2}.
$$

#### Two group, pre-test post-test design: repeated measures estimation

Another way to analyze the data from the same type of study design is to use repeated measures ANOVA. This is not to say that it's a good idea, but I've recently encountered a number of studies that do it (here's a recent example from [a highly publicized study in PLOS ONE](http://dx.doi.org/10.1371/journal.pone.0154075)--see Table 2). The studies I've seen typically report the sample means and variances in each group and at each time point, from which the difference in change scores can be calculated. Let $\bar{y}_{gt}$ and $s_{gt}^2$ denote the sample mean and sample variance in group $g = T, C$ at time $t = 0, 1$. The numerator of $d$ would then be calculated as 

$$
b = \left(\bar{y}_{T1} - \bar{y}_{T0}\right) - \left(\bar{y}_{C1} - \bar{y}_{C0}\right),
$$

which has sampling variance $\text{Var}(b) = 2(1 - \rho)\sigma^2\left(n_C + n_T \right) / (n_C n_T)$, where $\rho$ is the correlation between the pre-test and the post-test measures. Thus, the scaled standard error is

$$
\frac{se_b}{S} = \frac{2(1 - r)(n_C + n_T)}{n_C n_T}.
$$

As with ANCOVA, there are several potential options for calculating the denominator of $d$:

* Using the pooled sample variances on the post-test measures, with $\nu = n_C + n_T - 2$;
* Using the pooled sample variances on the pre-test measures, with $\nu = n_C + n_T - 2$; or
* Using the pooled sample variances at both time points and in both groups, i.e., 

    $$
    S^2 = \frac{(n_C - 1)(s_{C0}^2 + s_{C1}^2) + (n_T - 1)(s_{T0}^2 + s_{T1}^2)}{2(n_C + n_T - 2)},
    $$
    
    with $\nu = 2(n_C + n_T - 2) / (1 + r^2)$.

#### Randomized trial with longitudinal follow-up


#### Cluster-randomized trials


### References

Borenstein, M. (2009). Effect sizes for continuous data. In H. M. Cooper, L. V Hedges, & J. C. Valentine (Eds.), The Handbook of Research Synthesis and Meta-Analysis (pp. 221–236). New York, NY: Russell Sage Foundation.

Feingold, A. (2015). Confidence interval estimation for standardized effect sizes in multilevel and latent growth modeling. Journal of Consulting and Clinical Psychology, 83(1), 157–168. doi:10.1037/a0037721

Gleser, L. J., & Olkin, I. (2009). Stochastically dependent effect sizes. In H. Cooper, L. V. Hedges, & J. C. Valentine (Eds.), The Handbook of Research Synthesis and Meta-Analysis (2nd ed., pp. 357–376). New York, NY: Russell Sage Foundation.

Hedges, L. V. (2007). Effect sizes in cluster-randomized designs. Journal of Educational and Behavioral Statistics, 32(4), 341–370. doi:10.3102/1076998606298043

Hedges, L. V. (2011). Effect sizes in three-level cluster-randomized experiments. Journal of Educational and Behavioral Statistics, 36(3), 346–380. doi:10.3102/1076998610376617

Pustejovsky, J. E., Hedges, L. V, & Shadish, W. R. (2014). Design-comparable effect sizes in multiple baseline designs: A general modeling framework. Journal of Educational and Behavioral Statistics, 39(5), 368–393. doi:10.3102/1076998614547577

Viechtbauer, W. (2007). Approximate confidence intervals for standardized effect sizes in the two-independent and two-dependent samples design. Journal of Educational and Behavioral Statistics, 32(1), 39–60. doi:10.3102/1076998606298034