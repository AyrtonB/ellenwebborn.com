---
layout: post
title: "You wanna PEESE of d's?"
date: April 25, 2017
tags: [meta-analysis]
---

Publication bias---and more generally, outcome reporting bias---is now recognized as a critical threat to the validity of findings from research syntheses. In the areas with which I am most familiar (education and psychology), it has become more or less a requirement for research synthesis projects to conduct analyses to detect the presence of systematic outcome reporting biases. Some analyses go further by trying correct for its distorting effects on average effect size estimates. Widely known analytic techniques for doing so include Begg and Mazumdar's [rank-correlation test](https://dx.doi.org/10.2307/2533446), the Trim-and-Fill technique proposed by [Duval and Tweedie](https://dx.doi.org/10.2307/2669529), and [Egger regression](https://dx.doi.org/10.1136/bmj.315.7109.629) (in its [many variants](https://dx.doi.org/10.1186/1471-2288-9-2)). Another class of methods involves selection models (or weight function models), as proposed by [Hedges and Vevea](https://dx.doi.org/10.1007/BF02294384), [Vevea and Woods](https://dx.doi.org/10.1037/1082-989X.10.4.428), and others. As far as I can tell, these are well known among methodologists but very seldom applied due to their complexity and lack of ready-to-use software (though [an R package](https://CRAN.R-project.org/package=weightr) has recently become available).  More recent proposals include the PET-PEESE technique introduced by [Stanley and Doucouliagos](https://dx.doi.org/10.1002/jrsm.1095); Simonsohn, Nelson, and Simmon's [p-curve technique](https://dx.doi.org/10.1177/1745691614553988); Van Assen, Van Aert, and Wichert's [p-uniform](http://dx.doi.org/10.1037/met0000025), and others. The list of techniques grows by the day.

Among these methods, Egger regression, PET, and PEESE are superficially quite appealing due to their simplicity. These methods each involve estimating a fairly simple meta-regression model, using as the covariate the sampling variance of the effect size or some transformation thereof. PET uses the standard error of the effect size as the regressor; PEESE uses the sampling variance (i.e., the squared standard error); PET-PEESE involves first testing whether the PET estimate is statistically significant, using PEESE if it is or PET otherwise. The intercept from one of these regressions is the average effect size estimate from a study with zero sampling variance; the estimated intercept is used as a "bias-corrected" estimator of the population average effect. These methods are also appealing due to their extensibility. Because they are just meta-regressions, it is comparatively easy to extend them to meta-regression models that control for further covariates, to robust variance estimation to account for dependencies among effect size estimates, etc.

In [a recent blog post](http://datacolada.org/59), Uri Simonsohn reports some simulation evidence indicating that the PET-PEESE estimator can have large biases under certain conditions, _even in the absence of publication bias_. The simulations are based on standardized mean differences from two-group experiments and involve simulating collections of studies that include many with small sample sizes, as might be found in certain areas of psychology. On the basis of these performance assessments, he argues that this purported cure is actually worse than the disease---that PET-PEESE should _not_ be used in meta-analyses of psychological research because it performs to poorly to be trusted. In [a response to Uri's post](http://datacolada.org/wp-content/uploads/2017/04/Response-by-Joe-Hilgard-to-Colada-59.pdf), [Joe Hilgard](http://crystalprisonzone.blogspot.com/) suggests that some simple modifications to the method can improve its performance. Specifically, Joe suggests using a function of sample size as the covariate (in place of the standard error or sampling variance of $d$), and also using PET or PEESE as stand-alone estimators, rather than using them contingent on a significance test. 

In this post, I follow up Joe's suggestions while replicating and expanding upon Uri's simulations, to try and provide a fuller picture of the relative performance of these estimators. In brief, the simulations show that:

* Tests for small-sample bias that use PET or PEESE can have wildly incorrect type-I error rates in the absence of publication bias. Don't use them.
* The sample-size variants of PET and PEESE __do__ maintain the correct type-I error rates in the absence of publication bias. 
* The sample-size variants of PET and PEESE are __*exactly unbiased*__ in the absence of publication bias.
* However, these adjusted estimators still have a cost, being less precise than conventional fixed-effect or random-effects estimators. 
* In the presence of varying degrees of publication bias, the sample-size variant of PEESE does better than PET-PEESE as it is usually applied. But it's still not great. 

# Why use sample size?

To see why it makes sense to use a function of sample size as the covariate for PET-PEESE analyses, rather than using the standard error of the effect size estimate, let's look at the formulas. Say that we have a standardized mean difference estimate from a two-group design (without covariates) with sample sizes $n_0$ and $n_1$:

$$
d = \frac{\bar{y}_1 - \bar{y}_0}{s_p},
$$

where $\bar{y}_0$ and $\bar{y}_1$ are the sample means within each group and $s_p^2$ is the pooled sample variance. Following convention, we'll assume that the outcomes are normally distributed within each group, and the groups have common variance. The exact sampling variance of $d$ is a rather complicated formula, but one which can be approximated reasonably well as

$$
\text{Var}(d) \approx \frac{n_0 + n_1}{n_0 n_1} + \frac{\delta^2}{2(n_0 + n_1)},
$$

where $\delta$ is the _true_ standardized mean difference parameter. This formula is a delta-method approximation. The first term captures the variance of the numerator of $d$, so it gets at how precisely the _unstandardized_ difference in means is estimated. The second term captures the variance of the denominator of $d$, so it gets at how precisely the _scale_ of the outcome is estimated. The second term also involves the unknown parameter $\delta$, which must be estimated in practice. The conventional formula for the estimated sampling variance of $d$ substitutes $d$ in place of $\delta$:

$$
V = \frac{n_0 + n_1}{n_0 n_1} + \frac{d^2}{2(n_0 + n_1)}.
$$

In PET-PEESE analysis, $V$ or its square root is used as a covariate in a regression of the effect sizes, as a means of adjusting for publication bias. There are two odd things about this. First, publication bias is about the statistical significance of the group differences, but statistical significance __*does not depend on the scale of the outcome*__. The test of the null hypothesis of no differences between groups is __*not*__ based on $d / \sqrt{V}$. Instead, it is a function of the $t$ statistic:

$$
t = d / \sqrt{\frac{n_0 + n_1}{n_0 n_1}}.
$$

Consequently, it makes sense to use __*only the first term of $V$*__ as a covariate for purposes of detecting publication biases. 

The second odd thing is that $V$ is generally going to be correlated with $d$ because we have to use $d$ to calculate $V$. As [Joe explained in his response to Uri](http://datacolada.org/wp-content/uploads/2017/04/Response-by-Joe-Hilgard-to-Colada-59.pdf), this means that there will be a non-zero correlation between $d$ and $V$ (or between $d$ and $\sqrt{V}$) except in some very specific cases, even in the absence of any publication bias. Pretty funky. 

This second problem with regression tests for publication bias has been recognized for a while in the literature (e.g., [Macaskill, Walter, & Irwig, 2001](https://dx.doi.org/10.1002/sim.698); [Peters et al., 2006](https://dx.doi.org/10.1001/jama.295.6.676); [Moreno et al., 2009](https://dx.doi.org/10.1186/1471-2288-9-2)), but most of the work here has focused on other effect size measures, like odds ratios, that are relevant in clinical medicine. The behavior of these estimators might well differ for $d$'s because the dependence between the effect measure and its variance has a different structure. Below I'll investigate how this stuff works with standardized mean differences

# Simulation model

```{r setup, include = FALSE}

library(knitr)
opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(stringr)
library(tidyr)
library(dplyr)
library(ggplot2)

load("../files/PET-PEESE-Simulation-Results.Rdata")

```

The simulations are based on the following data-generating model, which closely follows [the structure that Uri used](http://datacolada.org/59):

* Per-cell sample size is generated as $n = 12 + B (n_{max} - 12)$, where $B \sim Beta(\alpha, \beta)$ and $n_{max}$ is the maximum observed sample size. We take $n_{max} = 50$ or 120$ and look at three sample size distributions:
    * $\alpha = \beta = 1$ corresponds to a uniform distribution on $[12,n_{max}]$;
    * $\alpha = 1, \beta = 3$ is a distribution with more small studies; and
    * $\alpha = 3, \beta = 1$ is a distribution with more large studies.
* True effects are simulated as $\delta \sim N(\mu, \sigma^2)$, for $\mu = 0, 0.1, 0.2, ..., 1.0$ and $\sigma = 0.0, 0.1, 0.2, 0.4$. Note that the values of $\sigma$ are _standard deviations_ of the true effects, with $\sigma = 0.0$ corresponding to the constant effect model and $\sigma = 0.4$ corresponding to rather substantial effect heterogeneity. 
* Standardized mean difference effect size estimates are generated as in a two-group between-subjects experiment with equal per-cell sample sizes. I do this by taking $d = D / \sqrt{S / [2(n - 1)]}$, where $D \sim N(\delta \sqrt{n / 2}, 1)$ and $S \sim \chi^2_{2(n - 1)}$. 
* Observed effects are filtered based on statistical significance. Let $p$ be the p-value corresponding to the observed $d$ and the one-tailed hypothesis test of $\delta \leq 0$. If $p < .025$, $d$ is observed with probability 1. If $p \geq .025$, then $d$ is observed with probability $\pi$. Noted that this mechanism corresponds to filtering based on two-sided hypothesis tests, where effects are filtered if they are statistically non-significant effects or statistically significant but in the wrong direction. I look at three scenarios:
    * $\pi = 1.0$ corresponds to no publication bias (all simulated effects are observed);
    * $\pi = 0.2$ corresponds to intermediate publication bias (some but not non-significant effects are observed); and
    * $\pi = 0.0$ corresponds to very strong publication bias (only statistically significant effects are observed).
* Each meta-analysis includes a total of $k = 100$ observed studies. Note that in scenarios with publication bias, more (sometimes many more) than 100 studies are generated in order to get 100 observed effects. 

For each simulated dataset, I calculated the following:

* the usual fixed-effect meta-analytic average (I skipped random effects for simplicity);
* the usual fixed-effect average, but based only on the 10 largest studies (I call this the "Top-10" estimator in the results);
* the PET estimator (including intercept and slope);
* the PEESE estimator (including intercept and slope);
* PET-PEESE, which is equal to the PEESE intercept if $H_0: \beta_0 \leq 0$ is significant at the 10% level, and is otherwise equal to the PET intercept (this definition follows [Stanley, 2017](https://dx.doi.org/10.1177/1948550617693062));
* the modified PET estimator, which I'll call "SPET" for "sample-size PET" (suggestions for better names welcome);
* the modified PEESE estimator, which I'll call "SPEESE"; and 

Simulation results are summarized across `r unique(params$reps)` replications. The R code for all this [lives here]({{ site.url}}/R/PET-PEESE-performance-simulations.R). Complete numerical results [live here]({{ site.url}}/files/PET-PEESE-Simulation-Results.Rdata)

# Results

```{r cleaning}

results <- 
  results %>%
  mutate(
    study_dist = ifelse(na == nb, "Uniform distribution of sample sizes", 
                        ifelse(na > nb, "More large studies", "More small studies")),
    study_dist = factor(study_dist, 
                        levels = c("More small studies","Uniform distribution of sample sizes","More large studies")),
    heterogeneity = paste("Between-study SD:", sd_effect),
    selection_level = factor(p_RR, levels = c(1, 0.2, 0), labels = c("No publication bias", "Intermediate publication bias", "Strong publication bias"))
  ) %>%
  select(-reps, -seed, -studies, -na, -nb, -n_min) %>%
  unnest() %>%
  mutate(
    RMSE = sqrt((est_M - mean_effect)^2 + est_V)
  )

```

### False-positive rates for publication bias detection

```{r rejection_rates}

Type_I_error_rates <- 
  results %>% 
  filter(coef != "(Intercept)", p_RR == 1) %>%
  select(-coef, -p_RR, -est_M, -est_V, -RMSE) %>%
  gather("rate","reject", starts_with("reject")) %>%
  mutate(rate = as.numeric(str_sub(rate,-3,-1)) / 1000)

rejection_rate_plot <- function(dat) {
  rate <- unique(dat$rate)
  ggplot(dat, aes(mean_effect, reject, linetype = estimator, color = factor(n_max))) + 
    geom_point() + geom_line() + 
    geom_hline(yintercept = rate, linetype = "dashed") + 
    facet_grid(heterogeneity ~ study_dist, scale = "free_y") + 
    coord_cartesian(ylim = c(0, 0.3)) + 
    theme_light() + 
    theme(legend.position = "bottom") + 
    labs(linetype = "", color = "Maximum n", x = "Mean effect size", y = "Rejection rate")
}
```

First, let's consider the performance of PET and PEESE as tests for detecting publication bias. Here, a statistically significant estimate for the coefficient on the SE (for PET) or sampling variance (for PEESE) is taken as evidence of small-sample bias. For that logic to hold, the tests should maintain the nominal error rates in the absence of publication bias. 

The figure below depicts the Type-I error rates of the PET and PEESE tests when $\pi = 1$ (so no publication bias at all), for a one-sided test of $H_0: \beta_1 \leq 0$ at the nominal level of $\alpha = .05$. Rejection rates are plotted for varying true mean effects, levels of heterogeneity, and sample size distributions. Separate colors are used for maximum sample sizes of 50 or 120.

```{r, PET-PEESE-rejection-rates, fig.width = 8, fig.height = 8}

Type_I_error_rates %>%
  filter(estimator %in% c("PET","PEESE"), rate == .050) %>%
  rejection_rate_plot()

```

Both tests are horribly mis-calibrated, tending to reject the null hypothesis far more often than they should. This happens because there is actually a non-zero correlation between $d$ and $V$, even in the absence of publication bias. Thus, it does not follow that rejecting $H_0: \beta_1 \leq 0$ implies rejection of the hypothesis that there is no publication bias. (Sorry, that's at least a triple negative!)

Here's the same graph, but using the SPET and SPEESE estimators:

```{r, SPET-SPEESE-rejection-rates, fig.width = 8, fig.height = 8}

Type_I_error_rates %>%
  filter(estimator %in% c("SPET","SPEESE"), rate == .050) %>%
  rejection_rate_plot()

```

Yes, this may be the world's most boring graph, but it does make clear that both the SPET and SPEESE tests maintain the correct Type-I error rate. (Any variation in rejection rates is just Monte Carlo error.) Thus, it seems pretty clear that if we want to test for small-sample bias, SPET or SPEESE should be used rather than PET or PEESE.

### Bias of bias-corrected estimators

Now let's consider the performance of these methods as estimators of the population mean effect. [Uri's analysis](http://datacolada.org/59) focused on the bias of the estimators, meaning the difference between the average value of the estimator (across repeated samples) and the true parameter. The plot below depicts the expected level of PET, PEESE, and PET-PEESE as a function of the true mean effect, using the uniform distribution of studies and a maximum sample size of $n = 50$: 

```{r bias-of-PET-PEESE, fig.width = 8, fig.height = 8}

estimator_performance <- 
  results %>%
  filter(coef == "(Intercept)") %>%
  select(-coef, -reject_025, -reject_050) %>%
  rename(maximum_n = n_max)
  
bias_plot <- function(dat) {
  subtitle <- paste0(unique(dat$study_dist), ", maximum sample size of ", unique(dat$maximum_n))
  ggplot(dat, aes(mean_effect, est_M, color = estimator, shape = estimator)) + 
    geom_point() + geom_line() + 
    geom_abline(slope = 1, intercept = 0) + 
    coord_cartesian(ylim = c(0, 1)) + 
    facet_grid(heterogeneity ~ selection_level) + 
    theme_light() + 
    theme(legend.position = "bottom") + 
    labs(
      title = "Expected value of effect size estimators",
      subtitle = subtitle,
      color = "", shape = "", 
      x = "Mean effect size", y = "Expected value of estimator"
    )
}

estimator_performance %>%
  filter(study_dist == "Uniform distribution of sample sizes", 
         maximum_n == 50, 
         estimator %in% c("PET","PEESE","PET-PEESE")) %>%
  bias_plot()
```

All three of these estimators are pretty bad in terms of bias. In the absence of publication bias, they consistently _under_-estimate the true mean effect. With intermediate or strong publication bias, PET and PET-PEESE have a consistent downward bias. As an unconditional estimator, PEESE tends to have a potisive bias when the true effect is small, but this decreases and becomes negative as the true effect increases. For all three estimators, bias increase as the degree of heterogeneity increases.

Here is how these estimators compare to the modified SPET, SPEESE, and SPET-SPEESE estimators, as well as to the usual fixed-effect average with no correction for publication bias:

```{r bias-of-SPET-SPEESE, fig.width = 8, fig.height = 8}

estimator_performance %>%
  filter(study_dist == "Uniform distribution of sample sizes", 
         maximum_n == 50, 
         estimator %in% c("FE-meta","PEESE","PET-PEESE","SPET","SPEESE","SPET-SPEESE")) %>%
  bias_plot()

```
```{r more-bias-of-SPET-SPEESE, fig.width = 8, fig.height = 8, fig.show = "hide"}

estimator_performance %>%
  filter(study_dist == "Uniform distribution of sample sizes", 
         maximum_n == 120, 
         estimator %in% c("FE-meta","PEESE","PET-PEESE","SPET","SPEESE","SPET-SPEESE")) %>%
  bias_plot()

estimator_performance %>%
  filter(study_dist == "More small studies", 
         maximum_n == 50, 
         estimator %in% c("FE-meta","PEESE","PET-PEESE","SPET","SPEESE","SPET-SPEESE")) %>%
  bias_plot()

estimator_performance %>%
  filter(study_dist == "More small studies", 
         maximum_n == 120, 
         estimator %in% c("FE-meta","PEESE","PET-PEESE","SPET","SPEESE","SPET-SPEESE")) %>%
  bias_plot()

estimator_performance %>%
  filter(study_dist == "More large studies", 
         maximum_n == 50, 
         estimator %in% c("FE-meta","PEESE","PET-PEESE","SPET","SPEESE","SPET-SPEESE")) %>%
  bias_plot()

estimator_performance %>%
  filter(study_dist == "More large studies", 
         maximum_n == 120, 
         estimator %in% c("FE-meta","PEESE","PET-PEESE","SPET","SPEESE","SPET-SPEESE")) %>%
  bias_plot()

```

In the left column, we see that SPET, SPEESE, and SPET-SPEESE are all exactly unbiased in the absence of selective publication (and so is regular old fixed effect meta-analysis, of course). In the middle and right columns, studies are selected based partially or fully on statistical significance, and things get messy. At small or moderate levels of between-study heterogeneity, PEESE, SPEESE, and SPET-SPEESE have pretty similar biases when the true mean effect is small, but SPEESE and SPET-SPEESE perform better as the true mean effect increases in size. (This makes sense because the bias induced by the correlation between $d$ and $V$ will be larger when the true effect sizes are larger.) However, if we compare PEESE versus SPEESE, there's not a consistent winner. For small true mean effects, PEESE actually has smaller biases than SPEESE. This seems to me to be nothing but a fortuitous accident, in that the bias induced by the correlation between $d$ and $V$ just happens to work in the right direction.  

These trends seem to largely hold for the other sample size distributions I examined too, although the biases of PEESE and PET-PEESE aren't as severe when the maximum sample size is larger. You can see for yourself here:

* [Uniform distribution of studies, maximum sample size of 120]({{site.url}}/figure/2017-04-25-PET-PEESE-performance/more-bias-of-SPET-SPEESE-1.png)
* [More small studies, maximum sample size of 50]({{site.url}}/figure/2017-04-25-PET-PEESE-performance/more-bias-of-SPET-SPEESE-2.png)
* [More small studies, maximum sample size of 120]({{site.url}}/figure/2017-04-25-PET-PEESE-performance/more-bias-of-SPET-SPEESE-3.png)
* [More large studies, maximum sample size of 50]({{site.url}}/figure/2017-04-25-PET-PEESE-performance/more-bias-of-SPET-SPEESE-4.png)
* [More large studies, maximum sample size of 120]({{site.url}}/figure/2017-04-25-PET-PEESE-performance/more-bias-of-SPET-SPEESE-5.png)

### Accuracy of bias-corrected estimators

```{r RMSE-plots, fig.width = 8, fig.height = 8}

RMSE_plot <- function(dat) {
  subtitle <- paste0(unique(dat$study_dist), ", maximum sample size of ", unique(dat$maximum_n))
  ggplot(dat, aes(mean_effect, RMSE, color = estimator, shape = estimator)) + 
    geom_point() + geom_line() + 
    coord_cartesian(ylim = c(0, 0.4)) + 
    facet_grid(heterogeneity ~ selection_level, scales = "free_y") + 
    theme_light() + 
    theme(legend.position = "bottom") + 
    labs(
      title = "Root mean squared error of effect size estimators",
      subtitle = subtitle,
      color = "", shape = "", 
      x = "Mean effect size", y = "RMSE"
    )
}

estimator_performance %>%
  filter(study_dist == "Uniform distribution of sample sizes", 
         maximum_n == 50, 
         estimator %in% c("FE-meta","PEESE","PET-PEESE","SPET","SPEESE","SPET-SPEESE")) %>%
  RMSE_plot()

```

```{r more-RMSE-plots, fig.width = 8, fig.height = 8, fig.show = "hide"}
estimator_performance %>%
  filter(study_dist == "Uniform distribution of sample sizes", 
         maximum_n == 120, 
         estimator %in% c("FE-meta","PEESE","PET-PEESE","SPET","SPEESE","SPET-SPEESE")) %>%
  RMSE_plot()

estimator_performance %>%
  filter(study_dist == "More small studies", 
         maximum_n == 50, 
         estimator %in% c("FE-meta","PEESE","PET-PEESE","SPET","SPEESE","SPET-SPEESE")) %>%
  RMSE_plot()

estimator_performance %>%
  filter(study_dist == "More small studies", 
         maximum_n == 120, 
         estimator %in% c("FE-meta","PEESE","PET-PEESE","SPET","SPEESE","SPET-SPEESE")) %>%
  RMSE_plot()

estimator_performance %>%
  filter(study_dist == "More large studies", 
         maximum_n == 50, 
         estimator %in% c("FE-meta","PEESE","PET-PEESE","SPET","SPEESE","SPET-SPEESE")) %>%
  RMSE_plot()

estimator_performance %>%
  filter(study_dist == "More large studies", 
         maximum_n == 120, 
         estimator %in% c("FE-meta","PEESE","PET-PEESE","SPET","SPEESE","SPET-SPEESE")) %>%
  RMSE_plot()


```

You can see for yourself here:

* [Uniform distribution of studies, maximum sample size of 120]({{site.url}}/figure/2017-04-25-PET-PEESE-performance/more-RMSE-plots-1.png)
* [More small studies, maximum sample size of 50]({{site.url}}/figure/2017-04-25-PET-PEESE-performance/more-RMSE-plots-2.png)
* [More small studies, maximum sample size of 120]({{site.url}}/figure/2017-04-25-PET-PEESE-performance/more-RMSE-plots-3.png)
* [More large studies, maximum sample size of 50]({{site.url}}/figure/2017-04-25-PET-PEESE-performance/more-RMSE-plots-4.png)
* [More large studies, maximum sample size of 120]({{site.url}}/figure/2017-04-25-PET-PEESE-performance/more-RMSE-plots-5.png)

# Further thoughts and caveats

