---
title: Sampling variance of Pearson's r in a two-level design
author: 'James'
date: '2018-04-10'
slug: variance-of-r-in-two-level-design
categories: []
tags:
  - effect sizes
  - meta-analysis
  - delta method
  - distribution theory
header:
  caption: ''
  image: ''
---

Consider Pearson's correlation coefficient, $r$, calculated from two variables $X$ and $Y$ with population correlation $\rho$. If one calculates $r$ from a simple random sample of $N$ observations, $\{x_i, y_i\}_{i=1}^N$, then the sampling variance of $r$ will be approximately

$$
\text{Var}(r) \approx \frac{1}{N}\left(1 - \rho^2\right)^2.
$$

But what if the observations are drawn from a multi-stage sample? If one uses the raw correlation between the observations (ignoring the multi-level structure), then the $r$ will actually be a weighted average of within-cluster and between-cluster correlations (Snijders & Bosker, 2012). Intuitively, I would expect that the sampling variance of the between-cluster correlation will be a function of the number of clusters (regardless of the number of observations per cluster), so the variance of $r$ from a multi-stage sample would not necessarily be the same as that froma  simple random sample. What is the sampling variance of $r$ in this design?

Let me be more precise here by formalizing the sampling process. Suppose that we have a sample with $m$ clusters, $n_j$ observations in cluster $j$, and total sample size $N = \sum_{j=1}^m n_j$. Assume that 

$$
\begin{aligned}
X_{ij} &= v^x_j + e^x_{ij} \\
Y_{ij} &= v^y_j + e^y_{ij},
\end{aligned}
$$

for $i=1,...,n_j$ and $j=1,...,m$, where

$$
\left[\begin{array}{c} v^x_j \\ v^y_j \end{array}\right] \sim N\left(\left[\begin{array}{c}0 \\ 0 \end{array}\right], \left[\begin{array}{cc}\omega_x^2 & \phi \omega_x \omega_y \\ \phi \omega_x \omega_y & \omega_y^2\end{array}\right]\right) \qquad \text{and}\qquad \left[\begin{array}{c} e^x_{ij} \\ e^y_{ij} \end{array}\right] \sim N\left(\left[\begin{array}{c}0 \\ 0 \end{array}\right], \left[\begin{array}{cc}\sigma_x^2 & \rho \sigma_x \sigma_y \\ \rho \sigma_x \sigma_y & \sigma_y^2\end{array}\right]\right)
$$

and the error terms are mutually independent unless otherwise noted. The raw Pearson's $r$ is calculated using the total variances and covariances around the grand mean:

$$
$$

To understand the sampling distribution of $r$ in this model, it helps to work with sums of squares between and within clusters. Let 
$$
\begin{aligned}
\bar{x}_j &= \frac{1}{n_j} \sum_{i=1}^{n_j} X_{ij} \\
\bar{\bar{x}} &= \frac{1}{N} \sum_{j=1}^m \sum_{i=1}^{n_j} X_{ij} = \frac{1}{N} \sum_{j=1}^m n_j \bar{x}_j \\
SS_{xy}^W &= \sum_{j=1}^m \sum_{i=1}^{n_j} \left(X_{ij} - \bar{x}_j\right) \left(Y_{ij} - \bar{y}_j\right) \\
SS_{xy}^B &= \sum_{j=1}^m n_j \left(\bar{x}_j - \bar{\bar{x}}\right) \left(\bar{y}_j - \bar{\bar{y}}\right),
\end{aligned}
$$
with $SS^W_{xx}$, $SS^B_{xx}$, $SS^W_{yy}$, and $SS^B_{yy}$ defined analogously.

Pearson's $r$ calculated across the entire sample of $N$ observations can be written as a weighted average of within- and between-cluster sample correlations, with weights that depend on the intra-class correlations of $X$ and $Y$. Specifically, 
$$
r = r_W \times \sqrt{(1 - c_x)(1 - c_y)} + r_B \times \sqrt{c_x c_y},
$$
where 
$$
\begin{aligned}
r_W &= \frac{SS^W_{xy}}{\sqrt{SS^W_{xx} SS^W_{yy}}}, \qquad & r_B &= \frac{SS^B_{xy}}{\sqrt{SS^B_{xx} SS^B_{yy}}} \\
c_x &= \frac{SS^B_{xx}}{SS^B_{xx} + SS^W_{xx}}, \qquad & c_y &= \frac{SS^B_{yy}}{SS^B_{yy} + SS^W_{yy}}.
\end{aligned}
$$
Following standard results from multivariate analysis (Searle, 2006, p. 352, or likewise), the within sums-of-squares have the following variances and covariances:
$$
\begin{aligned}
\text{E}\left(SS^W_{xx}\right) &= (N - m) \sigma_x^2 \\
\text{E}\left(SS^W_{yy}\right) &= (N - m) \sigma_y^2 \\
\text{E}\left(SS^W_{xy}\right) &= (N - m) \rho \sigma_x \sigma_y \\
\text{Var}\left(SS^W_{xx}\right) &= (N - m) \times 2 \sigma_x^4 \\ 
\text{Var}\left(SS^W_{yy}\right) &= (N - m) \times 2 \sigma_y^4 \\
\text{Var}\left(SS^W_{xy}\right) &= (N - m) \times (1 + \rho^2)\sigma_x^2 \sigma_y^2 \\
\text{Cov}\left(SS^W_{xx}, SS^W_{yy}\right) &= (N - m) \times 2 \rho^2 \sigma_x^2 \sigma_y^2 \\
\text{Cov}\left(SS^W_{xx}, SS^W_{xy}\right) &= (N - m) \times 2 \rho \sigma_x^3 \sigma_y \\
\text{Cov}\left(SS^W_{yy}, SS^W_{xy}\right) &= (N - m) \times 2 \rho \sigma_x \sigma_y^3.
\end{aligned}
$$
The between sums-of-squares are a little bit trickier, but can be derived using standard results on quadratic forms in normal random variables. Here they are: 
$$
\begin{aligned}
\text{E}\left(SS^B_{xx}\right) &= f_1 \omega_x^2 + (m - 1)\sigma_x^2 \\
\text{E}\left(SS^B_{yy}\right) &= f_1 \omega_y^2 + (m - 1)\sigma_y^2 \\
\text{E}\left(SS^B_{xy}\right) &= f_1 \phi \omega_x \omega_y + (m - 1)\rho\sigma_x\sigma_y \\
\text{Var}\left(SS^B_{xx}\right) &= 2\left[f_2 \omega_x^4 + 2 f_1 \sigma_x^2 \omega_x^2 + (m - 1)\sigma_x^4 \right] \\ 
\text{Var}\left(SS^B_{yy}\right) &= 2\left[f_2 \omega_y^4 + 2 f_1 \sigma_y^2 \omega_y^2 + (m - 1)\sigma_y^4\right] \\
\text{Var}\left(SS^B_{xy}\right) &=  f_2 (1 + \phi^2) \omega_w^2 \omega_y^2 + f_1 \left(\sigma_x^2 \omega_y^2 + \sigma_y^2 \omega_x^2 + 2 \rho \sigma_x \sigma_y \phi \omega_x \omega_y \right) + (m - 1) (1 + \rho^2) \sigma_x^2 \sigma_y^2 \\
\text{Cov}\left(SS^B_{xx}, SS^B_{yy}\right) &= 2\left[ f_2 \phi^2 \omega_x^2 \omega_y^2 + 2 f_1 \phi \omega_x \omega_y \rho \sigma_x \sigma_y + (m - 1) \rho^2 \sigma_x^2 \sigma_y^2 \right]\\
\text{Cov}\left(SS^B_{xx}, SS^B_{xy}\right) &= 2\left[ f_2 \phi \omega_x^3 \omega_y + f_1 \left(\omega_x^2 \rho \sigma_x \sigma_y + \phi \omega_x \omega_y \sigma_x^2\right) + (m - 1)\rho \sigma_x^3 \sigma_y \right] \\
\text{Cov}\left(SS^B_{yy}, SS^B_{xy}\right) &= 2\left[ f_2 \phi \omega_x \omega_y^3 + f_1 \left(\omega_y^2 \rho \sigma_x \sigma_y + \phi \omega_x \omega_y \sigma_y^2\right) + (m - 1)\rho \sigma_x \sigma_y^3 \right],
\end{aligned}
$$
where
$$
\begin{aligned}
f_1 &= N - \frac{1}{N}\sum_{j=1}^m n_j^2 \\
f_2 &= \sum_{j=1}^m n_j^2 - \frac{2}{N}\sum_{j=1}^m n_j^3 + \frac{1}{N^2} \left(\sum_{j=1}^m n_j^2 \right)^2.
\end{aligned}
$$

One way to proceed is by working with the total sums of squares. Because the between- and within-SS are independent, 
$$
\begin{aligned}
\text{E}\left(SS^T_{xx}\right) &= f_1 \omega_x^2 + (N - 1)\sigma_x^2 \\
\text{E}\left(SS^T_{yy}\right) &= f_1 \omega_y^2 + (N - 1)\sigma_y^2 \\
\text{E}\left(SS^T_{xy}\right) &= f_1 \phi \omega_x \omega_y + (N - 1)\rho\sigma_x\sigma_y \\
\text{Var}\left(SS^T_{xx}\right) &= 2\left[f_2 \omega_x^4 + 2 f_1 \sigma_x^2 \omega_x^2 + (N - 1)\sigma_x^4 \right] \\ 
\text{Var}\left(SS^T_{yy}\right) &= 2\left[f_2 \omega_y^4 + 2 f_1 \sigma_y^2 \omega_y^2 + (N - 1)\sigma_y^4\right] \\
\text{Var}\left(SS^T_{xy}\right) &=  f_2 (1 + \phi^2) \omega_w^2 \omega_y^2 + f_1 \left(\sigma_x^2 \omega_y^2 + \sigma_y^2 \omega_x^2 + 2 \rho \sigma_x \sigma_y \phi \omega_x \omega_y \right) + (N - 1) (1 + \rho^2) \sigma_x^2 \sigma_y^2 \\
\text{Cov}\left(SS^T_{xx}, SS^T_{yy}\right) &= 2\left[ f_2 \phi^2 \omega_x^2 \omega_y^2 + 2 f_1 \phi \omega_x \omega_y \rho \sigma_x \sigma_y + (N - 1) \rho^2 \sigma_x^2 \sigma_y^2 \right]\\
\text{Cov}\left(SS^T_{xx}, SS^T_{xy}\right) &= 2\left[ f_2 \phi \omega_x^3 \omega_y + f_1 \left(\omega_x^2 \rho \sigma_x \sigma_y + \phi \omega_x \omega_y \sigma_x^2\right) + (N - 1)\rho \sigma_x^3 \sigma_y \right] \\
\text{Cov}\left(SS^T_{yy}, SS^T_{xy}\right) &= 2\left[ f_2 \phi \omega_x \omega_y^3 + f_1 \left(\omega_y^2 \rho \sigma_x \sigma_y + \phi \omega_x \omega_y \sigma_y^2\right) + (N - 1)\rho \sigma_x \sigma_y^3 \right],
\end{aligned}
$$
