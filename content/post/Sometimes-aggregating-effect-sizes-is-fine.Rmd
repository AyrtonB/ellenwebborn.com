---
title: 'Sometimes, aggregating effect sizes is fine'
author: James
date: '2019-06-28'
slug: Sometimes-aggregating-effect-sizes-is-fine
categories: []
draft: true
tags:
  - effect sizes
  - meta-analysis
header:
  caption: ''
  image: ''
  preview: yes
---

In meta-analyses of psychology, education, and other social science research, it is very common that some of the studies meeting inclusion criteria report more than one relevant effect size. 
For example, in a meta-analysis of intervention effects on reading outcomes, some studies may have used multiple measures of reading outcomes (each of which meets inclusion criteria), or may have measured outcomes at multiple follow-up times; some studies might have also investigated more than one version of an intervention, and it might be of interest to include effect sizes comparing each version to the no-intervention control condition;
and it's even possible that some studies may have _all_ of these features, potentially contributing _lots_ of effect size estimates.

These situations create a technical challenge for conducting a meta-analysis. 
Because effect size estimates from the same study are correlated, it's not usually reasonable to use methods that are premised on each effect size estimate being independent (i.e., univariate methods). 
Instead, the analyst needs methods that take into account the dependencies among estimates coming from the same study. 
It used to be common to use ad hoc approaches for handling dependence, such as averaging the estimates together or selecting one estimate per study and then using univariate methods. 
More sophisticated, multivariate meta-analysis (MVMA) models that directly account for correlations among the effect size estimates had been developed but were challenging to implement and so rarely used (at least, that's my impression). 
More recently, techniques such as multi-level meta-analysis (MLMA) and robust variance estimation (RVE) have emerged, which account for dependencies while using all available effect size estimates and still being feasible to implement. 
These new techniques of MLMA, and RVE are starting to be adopted in practice, and it is not implausible that they will become the standard approach in psychological and educational meta-analysis within a few years. 

Given the extent of interest in MLMA and RVE, one might wonder: are the older ad hoc approaches _ever_ reasonable or appropriate? 
I think that some are, under certain circumstances. 
In this post I'll highlight one such circumstance, where aggregating effect size estimates is not only reasonable but leads to _exactly the same results_ as a multivariate model. This occurs when two conditions are met:

1. We are not interested in within-study heterogeneity of effects and
2. Any predictors included in the model only vary between studies (i.e., effect sizes from the same study all have the same values of the predictors).

In short, if all we care about is understanding between-study variation in effect sizes, then it is fine to aggregate them up to the study level.

# A model that's okay to average

To make this argument precise, let me lay out a model where it applies. 
For full generality, I'll consider a meta-regression model for a collection of $K$ studies, where study $k$ contributes $J_k \geq 1$ effect size estimates. 
Let $T_{jk}$ denote effect size estimate $j$ in study $k$, with sampling variance $S_{jk}^2$. 
Effect size estimates from study $k$ maybe be correlated at the sampling level, so that $\text{Cov}(T_{ik}, T_{jk}) = \rho_{ijk} S_{ik} S_{jk}$. 
Let $\mathbf{x}_k$ be a row vector of predictor variables for study $k$. 
Note that the predictors do not have a subscript $j$ because I'm assuming here that they are constant within a study. 
A multivariate meta-regression model for these data might be:
$$
T_{jk} = \mathbf{x}_k \boldsymbol\beta + u_k + e_{jk},
$$
where $u_k$ is a between-study random effect with variance $\tau^2$ and $e_{jk}$ is the sampling error for effect size $j$ from study $k$, assumed to have known variance $S_{jk}^2$. 
Errors from the same study are correlated, so $\text{Cov}(e_{ik}, e_{jk}) = \rho_{ijk} S_{ik} S_{jk}$. 
I will assume that the correlations are known, although in practice one might need to just take a guess about the degree of correlation, such as by assuming $\rho_{ijk} = 0.7$ for all pairs of estimates from each included study. 

This is a commonly considered model for dependent effect size estimates. 
In the paper that introduced RVE, Hedges, Tipton, and Johnson (2010) termed it the "correlated effects" model (implemented in `robumeta` as `model = "CORR"`, which is the default). 
Note that it also satisfies the conditions I outlined above: no within-study random effects, predictors that vary only between study. 
One could fit it easily using the `rma.mv()` function in the `metafor` package.

An alternative to this multivariate model would be to first average the effects within each study, then fit a univariate random effects model. 
Just how we do the averaging will matter: we'll need to use inverse-variance weighting. 
I'll start by describing the general case (which requires a little matrix algebra) and then commenting on a common special case (which is easier to express).
Let $\mathbf{T}_k$ be the $J_k \times 1$ vector of effect size estimates from study $k$. Let $\mathbf{S}_k$ be the $J_k \times J_k$ sampling covariance matrix for $\mathbf{T}_k$, and let $\mathbf{1}_k$ be a $J_k \times 1$ vector of 1s. The inverse-variance weighted average of the effects from study k can then be written as
$$
\bar{T}_k = V_k \mathbf{1}_k’ \mathbf{S}_k^{-1} \mathbf{T}_k, 
$$
where $V_k = 1 / (\mathbf{1}_k’ \mathbf{S}_k^{-1} \mathbf{1}_k)$, which is also the sampling variance of $\bar{T}_k$. 

After aggregating, a conventional, univariate random effects model for the averaged effect sizes would then be
$$
\bar{T}_k = \mathbf{x}_k \boldsymbol\beta + u_k + \bar{e}_k, 
$$
where $\text{Var}(u_k) = \tau^2$ and $\text{Var}(\bar{e}_k) = V_k$. 
This model can be fit using `rma.uni` from `metafor`. 
In fact, doing so will yield the same estimates of model parameters as fitting the multivariate model---for all intents and purposes, they are equivalent models. 
There are at several different ways to see that this equivalence holds. 
I'll offer three, from most practical to most theoretical.
(If you'd rather just take my word that this claim is true, feel free to skip down to the [last section](#so-what), where I comment on implications.)

# Computational equivalence

One good way to check the equivalence of the univariate and multivariate models is to apply both to a real dataset.

The main limitation of using real data here is generality---how can we be sure that these results aren't just a quirk of this particular dataset? Would we get the same results for _any_ dataset? 

# From multivariate to univariate model 

Here's another, somewhat more general perspective on the relationship between the models: the univariate model can be _derived_ directly from the multivariate one. Start with the multivariate model in matrix form:
$$
\mathbf{T}_k = \mathbf{x}_k \boldsymbol\beta \mathbf{1}_k + u_k \mathbf{1}_k + \mathbf{e}_k,
$$
where $\mathbf{e}_k$ is the vector of sampling errors for study $k$, with $\text{Var}(\mathbf{e}_k) = \mathbf{S}_k$. Pre-multiply both sides by $V_k \mathbf{1}_k’ \mathbf{S}_k^{-1}$ to get
$$
\begin{aligned}
V_k \mathbf{1}_k’ \mathbf{S}_k^{-1} \mathbf{T}_k &= V_k \left(\mathbf{1}_k’ \mathbf{S}_k^{-1} \mathbf{1}_k\right) \mathbf{x}_k \boldsymbol\beta + u_k V_k \left(\mathbf{1}_k’ \mathbf{S}_k^{-1} \mathbf{1}_k\right) + V_k \mathbf{1}_k’ \mathbf{S}_k^{-1} \mathbf{e}_k \\
\bar{T}_k &= \mathbf{x}_k \boldsymbol\beta + u_k + \bar{e}_k,
\end{aligned}
$$
where $\text{Var}(\bar{e}_k) = V_k \mathbf{1}_k’ \mathbf{S}_k^{-1} \mathbf{S}_k \mathbf{S}_k^{-1} \mathbf{1}_k V_k = V_k$, just as in the univariate model. 

This demonstrates that the parameters of the two models are the same quantities—that is, both models are estimating the same thing. But that would also hold if we used _any_ weighted average of $\mathbf{T}_k$---it needn't be inverse-variance. The only thing that would be different is $\text{Var}(\bar{e}_k)$. To fully establish the equivalence of the two models, we'll need to look at the likelihoods of each model.

# Equivalence of likelihoods 

Typically multivariate meta-analysis models are estimated by full maximum likelihood (FML) or restricted maximum likelihood methods. FML and RML are also commonly used for univariate meta-analysis. With these methods, estimates are obtained as the parameter values that maximize the log likelihood (or the restricted likelihood for RML). Therefore, we can establish the exact equivalence of parameter estimates by showing that the log likelihood of the univariate and multivariate models differ by a constant value (so that the location of the maxima are identical). 

## Full likelihood 

For the univariate model, the log-likelihood contribution of study $k$:
$$
l^{U}_k\left(\boldsymbol\beta, \tau^2\right) = -\frac{1}{2} \log\left(\tau^2 + V_k\right) - \frac{1}{2} \frac{\left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right)^2}{\tau^2 + V_k}.
$$
For the multivariate model, the log-likelihood contribution of study $k$ is:
$$
l^{MV}_k\left(\boldsymbol\beta, \tau^2\right) = -\frac{1}{2} A -\frac{1}{2} B
$$
where 
$$
A = \log\left|\tau^2\mathbf{1}_k\mathbf{1}_k' + \mathbf{S}_k\right| 
$$
and 
$$
B = \left(\mathbf{T}_k - \mathbf{x}_k \boldsymbol\beta \mathbf{1}_k\right)' \left(\tau^2\mathbf{1}_k\mathbf{1}_k' + \mathbf{S}_k\right)^{-1} \left(\mathbf{T}_k - \mathbf{x}_k \boldsymbol\beta \mathbf{1}_k\right).
$$
The term $A$ can be rearranged as
$$
A = \log\left|\left(\tau^2\mathbf{1}_k\mathbf{1}_k'\mathbf{S}_k^{-1} + \mathbf{I}_k\right) \mathbf{S}_k\right|
$$
where $\mathbf{I}_k$ is a $J_k \times J_k$ identity matrix. One of the properties of determinants is that the determinant of a product of two matrices is equal to the product of the determinants. Another is that, for two vectors $\mathbf{u}$ and $\mathbf{v}$, $\left|\mathbf{I} + \mathbf{u}\mathbf{v}'\right| = 1 + \mathbf{v}'\mathbf{u}$. Applying both of these properties, it follows that 
$$
\begin{aligned}
A &= \log\left|\left(\tau^2\mathbf{1}_k\mathbf{1}_k'\mathbf{S}_k^{-1} + \mathbf{I}_k\right) \mathbf{S}_k\right| \\
&= \log \left( \left|\mathbf{I}_k + \tau^2\mathbf{1}_k\mathbf{1}_k'\mathbf{S}_k^{-1}\right| \left|\mathbf{S}_k\right|\right) \\
&= \log \left(1 + \frac{\tau^2}{V_k}\right) + \log \left|\mathbf{S}_k\right| \\
&= \log(\tau^2 + V_k) - \log(V_k) + \log \left|\mathbf{S}_k\right|.
\end{aligned}
$$
The $B$ term takes a little more work. 
From [the Sherman-Morrison identity](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula), we have that: 
$$
\left(\tau^2\mathbf{1}_k\mathbf{1}_k' + \mathbf{S}_k\right)^{-1} = \mathbf{S}_k^{-1} - \mathbf{S}_k^{-1} \mathbf{1}_k \left(\frac{1}{\tau^2} + \frac{1}{V_k}\right)^{-1} \mathbf{1}_k'\mathbf{S}_k^{-1},
(\#eq:Sherman)
$$
by which it follows that
$$
\mathbf{1}_k'\left(\tau^2\mathbf{1}_k\mathbf{1}_k' + \mathbf{S}_k\right)^{-1}\mathbf{1}_k = \frac{1}{\tau^2 + V_k}.
(\#eq:inversevariance)
$$
Now, rearrange the $B$ term to get
$$
\begin{aligned}
B &= \left[\mathbf{T}_k - \bar{T}_k \mathbf{1}_k + \left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right) \mathbf{1}_k\right]' \left(\tau^2\mathbf{1}_k\mathbf{1}_k' + \mathbf{S}_k\right)^{-1} \left[\mathbf{T}_k - \bar{T}_k \mathbf{1}_k + \left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right) \mathbf{1}_k\right] \\
&= B_1 + 2 B_2 + B_3
\end{aligned}
$$
where
$$
\begin{aligned}
B_1 &= \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)' \left(\tau^2\mathbf{1}_k\mathbf{1}_k' + \mathbf{S}_k\right)^{-1} \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right) \\
B_2 &= \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)' \left(\tau^2\mathbf{1}_k\mathbf{1}_k' + \mathbf{S}_k\right)^{-1} \mathbf{1}_k \left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right) \\
B_3 &= \left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right) \mathbf{1}_k' \left(\tau^2\mathbf{1}_k\mathbf{1}_k' + \mathbf{S}_k\right)^{-1} \mathbf{1}_k \left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right) \\
&= \frac{\left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right)^2}{\tau^2 + V_k}
\end{aligned}
$$
and the last equality follows from \@ref(eq:inversevariance). Applying \@ref(eq:Sherman) to $B_1$,
$$
\begin{aligned}
B_1 &= \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)' \left[\mathbf{S}_k^{-1} - \mathbf{S}_k^{-1} \mathbf{1}_k \left(\frac{1}{\tau^2} + \frac{1}{V_k}\right)^{-1} \mathbf{1}_k'\mathbf{S}_k^{-1}\right] \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right) \\ 
&= \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)' \mathbf{S}_k^{-1}\left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right) - \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)' \mathbf{S}_k^{-1} \mathbf{1}_k \left(\frac{1}{\tau^2} + \frac{1}{V_k}\right)^{-1} \mathbf{1}_k'\mathbf{S}_k^{-1}\left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right) \\
&= \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)' \mathbf{S}_k^{-1}\left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right).
\end{aligned}
$$
The second term drops out because $\left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)' \mathbf{S}_k^{-1} \mathbf{1}_k = \bar{T}_k / V_k - \bar{T}_k / V_k = 0$. Along similar lines,
$$
\begin{aligned}
B_2 &= \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)' \left[\mathbf{S}_k^{-1} - \mathbf{S}_k^{-1} \mathbf{1}_k \left(\frac{1}{\tau^2} + \frac{1}{V_k}\right)^{-1} \mathbf{1}_k'\mathbf{S}_k^{-1}\right] \mathbf{1}_k \left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right) \\ 
&= \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)' \mathbf{S}_k^{-1}\mathbf{1}_k \left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right) - \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)' \mathbf{S}_k^{-1} \mathbf{1}_k \left(\frac{1}{\tau^2} + \frac{1}{V_k}\right)^{-1} \mathbf{1}_k'\mathbf{S}_k^{-1}\mathbf{1}_k \left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right) \\
&= 0.
\end{aligned}
$$
Thus, the full $B$ term reduces to
$$
B = \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)' \mathbf{S}_k^{-1}\left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right) + \frac{\left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right)^2}{\tau^2 + V_k}
$$
and the multivariate log likelihood contribution is
$$
\begin{aligned}
l^{MV}_k\left(\boldsymbol\beta, \tau^2\right) &= -\frac{1}{2} \log(\tau^2 + V_k) + \frac{1}{2} \log(V_k) - \frac{1}{2}\log \left|\mathbf{S}_k\right| - \frac{1}{2} \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)' \mathbf{S}_k^{-1}\left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right) -\frac{1}{2} \frac{\left(\bar{T}_k - \mathbf{x}_k \boldsymbol\beta\right)^2}{\tau^2 + V_k} \\
&= l^U_k\left(\boldsymbol\beta, \tau^2\right) + \frac{1}{2} \log(V_k) - \frac{1}{2}\log \left|\mathbf{S}_k\right| - \frac{1}{2} \left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right)' \mathbf{S}_k^{-1}\left(\mathbf{T}_k - \bar{T}_k \mathbf{1}_k\right).
\end{aligned}
$$
The last three terms depend on the data ($\mathbf{T}_k$ and $\mathbf{S}_k$) but not on the parameters $\boldsymbol\beta$ or $\tau^2$. Therefore, the univariate and multivariate likelihoods will be maximized at the same parameter values, i.e., the FML estimators are identical.

## Restricted likelihood 

In practice, it is more common to use RML estimation rather than FML.
The RML estimators maximize a different objective function that includes the full likelihood, plus an additional term. The RML objective function for the univariate model is
$$
\sum_{k=1}^K l^U_k(\boldsymbol\beta, \tau^2) - \frac{1}{2} R^U(\tau^2)
$$
where 
$$
R^U(\tau^2) = \log \left|\sum_{k=1}^k\frac{\mathbf{x}_k' \mathbf{x}_k}{\tau^2 + V_k} \right|.
$$
For the multivariate model, the RML objective is
$$
\sum_{k=1}^K l^{MV}_k(\boldsymbol\beta, \tau^2) - \frac{1}{2} R^{MV}(\tau^2).
$$
where
$$
\begin{aligned}
R^{MV}(\tau^2) &= \log \left|\sum_{k=1}^k \mathbf{x}_k'\mathbf{1}_k'\left(\tau^2\mathbf{1}_k\mathbf{1}_k' + \mathbf{S}_k\right)^{-1}\mathbf{1}_k \mathbf{x}_k \right|\\
&= \log \left|\sum_{k=1}^k\frac{\mathbf{x}_k' \mathbf{x}_k}{\tau^2 + V_k} \right| \\
&= R^U(\tau^2)
\end{aligned}
$$
because of \@ref(eq:inversevariance). Thus, the univariate and multivariate models also have the same RML estimators.

# So what?

blahbeddy blah blah.
